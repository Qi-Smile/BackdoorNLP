{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我找这个数据集好像有点问题，还有title，我把title和正文内容做了一下合并\n",
    "train_data = pd.read_csv('data/ag/train.csv')\n",
    "train_data.rename(columns={'Class Index': 'labels'}, inplace=True)\n",
    "train_data['text'] = train_data['Title']+' '+train_data['Description']\n",
    "train_data = train_data.drop(columns=['Description', 'Title'])\n",
    "train_data = train_data[['text', 'labels']]\n",
    "class_number = train_data['labels'].nunique()\n",
    "train_data['labels'] = train_data['labels'] - 1\n",
    "train_data.to_csv('data/ag/train_drop_title.csv',index=False)\n",
    "\n",
    "test_data = pd.read_csv('data/ag/train.csv')\n",
    "test_data.rename(columns={'Class Index': 'labels'}, inplace=True)\n",
    "test_data['text'] = test_data['Title']+' '+test_data['Description']\n",
    "test_data = test_data.drop(columns=['Description', 'Title'])\n",
    "test_data = test_data[['text', 'labels']]\n",
    "test_data['labels'] = test_data['labels'] - 1\n",
    "train_data.to_csv('data/ag/test_drop_title.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff771f58a082448faea2b46bf504ff20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0299b227b6934f0eac6e36c38b8105a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 108000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 12000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 120000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_csv = load_dataset('csv',data_files='data/ag/train_drop_title.csv',split='train')\n",
    "dataset_size = len(train_csv)\n",
    "train_size = int(dataset_size*0.9)\n",
    "train_dataset = train_csv.select(range(train_size))\n",
    "dev_dataset = train_csv.select(range(train_size,dataset_size))\n",
    "test_dataset = load_dataset('csv',data_files='data/ag/test_drop_title.csv',split='train')\n",
    "\n",
    "print(train_dataset)\n",
    "print(dev_dataset)\n",
    "print(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "国内使用huggingface是有问题的，离线使用预训练模型参数参考\n",
    "[link]'https://zhuanlan.zhihu.com/p/475260268'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at pretrained_model/bert/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('pretrained_model/bert/', num_labels=class_number)\n",
    "tokenizer = AutoTokenizer.from_pretrained('pretrained_model/bert/')\n",
    "MAX_LENGTH = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb0e4df4e7c4418a24397ef6af852a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2094e0ad8cfd411bbac44e1e31456d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02911eea8e774c0496309e7e3edf5255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 108000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 12000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 120000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 对数据集做分词预处理，把所有序列补充或截断到256个token\n",
    "train_dataset = train_dataset.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "dev_dataset = dev_dataset.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "test_dataset = test_dataset.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "# 处理成pytorch的格式\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "dev_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "print(train_dataset)\n",
    "print(dev_dataset)\n",
    "print(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/sfl/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/sfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5064' max='5064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5064/5064 01:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.124300</td>\n",
       "      <td>0.252292</td>\n",
       "      <td>0.919583</td>\n",
       "      <td>0.919811</td>\n",
       "      <td>0.920515</td>\n",
       "      <td>0.919940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.249654</td>\n",
       "      <td>0.920417</td>\n",
       "      <td>0.920633</td>\n",
       "      <td>0.921302</td>\n",
       "      <td>0.920654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>0.263735</td>\n",
       "      <td>0.919250</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.920789</td>\n",
       "      <td>0.919602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/sfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ubuntu/miniconda3/envs/sfl/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEIVCES\"]=\"\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    no_cuda=False,\n",
    "    load_best_model_at_end=True,\n",
    "    # eval_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated   Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_out = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
