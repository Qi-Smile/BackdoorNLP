{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notes will show you how to use BERT for text-classification. Some highlights:\n",
    "* dataloaders in pytorch,\n",
    "* BERT from pytorch-transformers,\n",
    "* freezing layers,\n",
    "* learning rate schedulers, optimizers and gradient clipping,\n",
    "* mixed precision training,\n",
    "* logging your training.\n",
    "\n",
    "Resources:\n",
    "* [a good tutorial from C. McCormick](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n",
    "* [the original BERT paper](https://arxiv.org/abs/1810.04805)\n",
    "* [the main library we use is pytorch-tranformers from huggingface](https://github.com/huggingface/pytorch-transformers)\n",
    "* ['Fine-tuning BERT for text classification' by Sun et. al](https://arxiv.org/abs/1905.05583)\n",
    "* [a post on compressing and speeding-up large models](https://blog.rasa.com/compressing-bert-for-faster-prediction-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm, trange, tqdm_notebook\n",
      "File \u001b[0;32m~/miniconda3/envs/sfl/lib/python3.10/site-packages/keras/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _tf_keras\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[0;32m~/miniconda3/envs/sfl/lib/python3.10/site-packages/keras/_tf_keras/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "File \u001b[0;32m~/miniconda3/envs/sfl/lib/python3.10/site-packages/keras/_tf_keras/keras/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks\n",
      "File \u001b[0;32m~/miniconda3/envs/sfl/lib/python3.10/site-packages/keras/activations/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
      "File \u001b[0;32m~/miniconda3/envs/sfl/lib/python3.10/site-packages/keras/src/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[0;32m~/miniconda3/envs/sfl/lib/python3.10/site-packages/keras/src/activations/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exponential\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gelu\n",
      "File \u001b[0;32m~/miniconda3/envs/sfl/lib/python3.10/site-packages/keras/src/activations/activations.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "File \u001b[0;32m~/miniconda3/envs/sfl/lib/python3.10/site-packages/keras/src/backend/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m any_symbolic_tensors\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_keras_tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/sfl/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_name\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.KerasTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mKerasTensor\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sfl/lib/python3.10/site-packages/keras/src/utils/tree.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Register backend-specific node classes\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_structures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ListWrapper\n\u001b[1;32m     14\u001b[0m     optree\u001b[38;5;241m.\u001b[39mregister_pytree_node(\n\u001b[1;32m     15\u001b[0m         ListWrapper,\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: (x, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m metadata, children: ListWrapper(\u001b[38;5;28mlist\u001b[39m(children)),\n\u001b[1;32m     18\u001b[0m         namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.tree.is_nested\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_nested\u001b[39m(structure):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_transformers import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle, os, shutil\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various pretrained models are available in pytorch-transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#          Model          | Tokenizer          | Pretrained weights shortcut\n",
    "MODELS = [(BertModel,       BertTokenizer,      'bert-base-uncased'),\n",
    "          (OpenAIGPTModel,  OpenAIGPTTokenizer, 'openai-gpt'),\n",
    "          (GPT2Model,       GPT2Tokenizer,      'gpt2'),\n",
    "          (TransfoXLModel,  TransfoXLTokenizer, 'transfo-xl-wt103'),\n",
    "          (XLNetModel,      XLNetTokenizer,     'xlnet-base-cased'),\n",
    "          (XLMModel,        XLMTokenizer,       'xlm-mlm-enfr-1024'),\n",
    "          (RobertaModel,    RobertaTokenizer,   'roberta-base')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and even BERT has many variations, the first being the base model and the rest are equipped with different heads for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n",
    "                      BertForSequenceClassification, BertForMultipleChoice, BertForTokenClassification,\n",
    "                      BertForQuestionAnswering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModelTokenizer(num_labels = 2, language = 'english'):\n",
    "    model_class = BertForSequenceClassification\n",
    "    tokenizer_class = BertTokenizer\n",
    "    if language == 'english':\n",
    "        pretrained_weights = 'bert-base-uncased'\n",
    "    if language == 'german':\n",
    "        pretrained_weights = 'bert-base-german-cased'\n",
    "\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights, do_lower_case=False)\n",
    "    model = model_class.from_pretrained(pretrained_weights, num_labels)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = loadModelTokenizer(4, 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass model to the GPU already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting model parameters and accessing individual layers is not too bad using the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model, trainable_only = True):\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_layers(model, verbose=False):\n",
    "    print('Number of layers: {}'.format(len(list(model.children()))))\n",
    "    if verbose:\n",
    "        for c in model.children():\n",
    "            print(str(c)[:50]+'...')\n",
    "\n",
    "def get_child(model, *arg):\n",
    "    res = model\n",
    "    for i in arg:\n",
    "        res = list(res.children())[i]\n",
    "    return res\n",
    "\n",
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers(model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go into BERT: it has three main parts, the first being BertEmbeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = get_child(model, 0, 0)\n",
    "b0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part is the middle: BertEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = get_child(model, 0, 1)\n",
    "b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latter has 12 layers and a gazillion parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, a simple BertPooler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = get_child(model, 0, 2)\n",
    "b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a custom freezer later that only lets part of BERT train apart from the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_freezer(model):\n",
    "    unfreeze_model(model)\n",
    "    print('All parameters unfreezed: {}'.format(count_parameters(model)))\n",
    "\n",
    "    ## freeze whole BertLayer\n",
    "    for c in model.children():\n",
    "        if str(c).startswith('Bert'):\n",
    "            freeze_model(c)\n",
    "            \n",
    "    ## unfreeze top 3 layer in BertEncoder\n",
    "    bert_encoder = get_child(model, 0, 1, 0)\n",
    "    for i in range(1, 4):\n",
    "        m = bert_encoder[-i] \n",
    "        print('Unfreezing: {}'.format(m))\n",
    "        unfreeze_model(m)\n",
    "        \n",
    "    ## unfreeze Pooling layer\n",
    "    bert_pooling = get_child(model, 0, 2)\n",
    "    unfreeze_model(bert_pooling)\n",
    "\n",
    "    print('Trainable parameters: {}'.format(count_parameters(model, True)))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_freezer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## put the train and text csv to PATH\n",
    "DATA_PATH = '...'\n",
    "\n",
    "def read_AGNews(DATA_PATH):\n",
    "    train_df = pd.read_csv(DATA_PATH+'train.csv', header=None)\n",
    "    train_df.rename(columns={0: 'label',1: 'title', 2:'text'}, inplace=True)\n",
    "    train_df = pd.concat([train_df, pd.get_dummies(train_df['label'],prefix='label')], axis=1)\n",
    "    \n",
    "    test_df = pd.read_csv(DATA_PATH+'test.csv', header=None)\n",
    "    test_df.rename(columns={0: 'label',1: 'title', 2:'text'}, inplace=True)\n",
    "    test_df = pd.concat([test_df, pd.get_dummies(test_df['label'],prefix='label')], axis=1)\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = read_AGNews(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.text.apply(lambda x: len(x)).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document lengths will be important in a second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the text for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do the following:\n",
    "* apply the pre-trained sub-word tokenizer that comes with BERT,\n",
    "* append special tokens for classification at the beginning and end of each sample,\n",
    "* make sure that each sample sequence of tokens has length at most 512 (that's the max input length for BERT).\n",
    "\n",
    "We will pad everything to uniform length to make life easier (torch.tensor will throw errors otherwise at later steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizer.encode() will do the tokenization and converting into subword IDs in a single step\n",
    "\n",
    "tokenizer.encode('a dog is not a table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cutting and padding\n",
    "def padSentenceForBert(sent, cls_id, sep_id, lg):\n",
    "    return [cls_id]+sent[:lg-2]+[sep_id] + [0]*(lg - len(sent[:lg-2])-2) \n",
    "        \n",
    "def padAndTokenizeForBert(sentences, lg):\n",
    "    cls_id = tokenizer.convert_tokens_to_ids(tokenizer.cls_token)\n",
    "    sep_id = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)\n",
    "    return [padSentenceForBert(tokenizer.encode(sent), cls_id, sep_id, lg) for sent in tqdm_notebook(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = padAndTokenizeForBert(train_df.text, 512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = padAndTokenizeForBert(test_df.text, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful, the **labels need to be from 0 to number_of_classes**. In our case, this is simply done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df.label.values - 1\n",
    "test_labels = test_df.label.values - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure that the model ignores the padding, so we define masks that will be fed to the BERT model as well (1 stands for a real token and 0 for padding coordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masker(input_ids):\n",
    "    return [[float(i>0) for i in seq] for seq in input_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = masker(train_ids)\n",
    "test_mask = masker(test_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation split for training data and masks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.1\n",
    "train_ids, val_ids, train_labels, val_labels, train_mask, val_mask = train_test_split(train_ids, train_labels, train_mask,  \n",
    "                                                                                random_state=42, test_size=SPLIT_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ids), len(train_labels), len(train_mask), len(val_ids), len(val_labels), len(val_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding the data to BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch has great dataset and dataloader classes that make life easier, especially when the data does not fit into memory. At the same time we construct the loader, we start turning our id sequences to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def loader(ids, mask, labels, mode = 'random'):\n",
    "    ids = torch.tensor(ids)\n",
    "    mask = torch.tensor(mask)\n",
    "    labels = torch.tensor(labels)\n",
    "    train_data = TensorDataset(ids, mask, labels)\n",
    "    if mode == 'random': \n",
    "        train_sampler = RandomSampler(train_data)\n",
    "    else:\n",
    "        train_sampler = SequentialSampler(train_data)\n",
    "    return DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = loader(train_ids, train_mask, train_labels)\n",
    "val_loader = loader(val_ids, val_mask, val_labels, mode = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look inside these data loaders as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, mask, label = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids.size(), mask.size(), label.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the batch of 32 samples, each of length 512."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT sequence classifier outputs logits which need to be turned into probabilities (softmax) before fed into the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, auc, precision_score, recall_score\n",
    "\n",
    "## alternatively: from scipy.special import softmax\n",
    "\n",
    "def softmax(logits):\n",
    "    return np.array([a/b for a,b in zip(np.exp(logits), np.sum(np.exp(logits), axis=1))])\n",
    "\n",
    "def softmax_accuracy(labels, logits):\n",
    "    preds = np.argmax(softmax(logits), axis=1)\n",
    "    return accuracy_score(labels, preds)\n",
    "\n",
    "def softmax_rocauc(labels, logits):\n",
    "    preds = np.argmax(softmax(logits), axis=1)\n",
    "    return roc_auc_score(labels, preds)\n",
    "\n",
    "def save_checkpoint(state, is_best, logdir = ''):\n",
    "    statefilename = logdir+'last_checkpoint.pt'\n",
    "    torch.save(state, statefilename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(statefilename, logdir+'model_best.pt')\n",
    "\n",
    "def eval_metrics(metrics, labels, preds):\n",
    "    result = dict()\n",
    "    for name, metric in metrics.items():\n",
    "        result[name] = metric(labels, preds)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a train-method which takes the model, optimizer and learning rate scheduler with a hyperparameter dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['epochs'] = 4\n",
    "params['max_grad'] = 1\n",
    "params['base_lr'] = 1e-3\n",
    "params['metrics'] = {'accuracy': softmax_accuracy, 'roc_auc': softmax_rocauc}\n",
    "\n",
    "## where to save the models (they can take a few Gb if all the params are training)\n",
    "LOG_PATH = '...'\n",
    "\n",
    "params['logdir'] = LOG_PATH + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"/\"\n",
    "\n",
    "params['train_dataloader'] = train_loader\n",
    "params['val_dataloader'] = val_loader\n",
    "\n",
    "num_total_steps = params['epochs']*len(params['train_dataloader'])\n",
    "warmup_proportion = 0.1\n",
    "num_warmup_steps = int(num_total_steps*warmup_proportion)\n",
    "    \n",
    "optimizer = AdamW(model.parameters(), lr=params['base_lr'], correct_bias=False) \n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_total_steps) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the train-method that I still need to modularize:\n",
    "* we set the model and opt. to mixed precision for faster training,\n",
    "* the main for-loop goes over the epochs:\n",
    "    * first, we loop over each batch, calculate the loss and back propagate,\n",
    "    * second, we evaluate the model on the validation set with respect to our metrics,\n",
    "    * last, we save the current model and update the best_model (if needed).\n",
    "    \n",
    "The method will return the history dictionary (much like Keras) with a bunch of nice metrics we can visualize.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mops, params, do_amp = True, test_mode=False, warm = False):\n",
    "    \n",
    "    model, optimizer, scheduler = mops\n",
    "    print('Trainable params in the model: {}\\n'.format(count_parameters(model)))\n",
    "    \n",
    "    ## metrics and logging\n",
    "    metrics = params['metrics']\n",
    "    os.makedirs(params['logdir'], exist_ok=True)\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "  \n",
    "    ## If new training started we reset the mixed-precision training:\n",
    "    if not warm and do_amp:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\", \n",
    "                                              keep_batchnorm_fp32=True, loss_scale=\"dynamic\")\n",
    "\n",
    "\n",
    "    for epoch in range(params['epochs']):\n",
    "        print(\"Epoch {}:\".format(epoch+1))\n",
    "        ## set training mode!\n",
    "        model.train()\n",
    "\n",
    "        ## Tracking loss during the epoch\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        ## Train the data for one epoch\n",
    "\n",
    "        for batch in tqdm_notebook(params['train_dataloader']):\n",
    "            \n",
    "            ## Add batch to GPU and unpack\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            ## Clear out the gradients (by default they accumulate) and forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, \n",
    "                           labels=b_labels)\n",
    "            loss = output[0]\n",
    "\n",
    "            ## Backward pass\n",
    "            if do_amp:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            ## Update parameters and clip\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), params['max_grad'])  \n",
    "\n",
    "\n",
    "            ## logging\n",
    "            history['train_loss_per_batch'].append(loss.item()) \n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            \n",
    "            ## for testing, break early\n",
    "            if test_mode and nb_tr_steps > 20:\n",
    "                break\n",
    "        \n",
    "        ## logging\n",
    "        history['train_loss_per_epoch'].append(tr_loss/nb_tr_steps)\n",
    "        \n",
    "        print(\"Train loss: {}\".format(history['train_loss_per_epoch'][-1]))\n",
    "\n",
    "        \n",
    "        print('Validation...')\n",
    "\n",
    "        val_labels, val_logits = testEval(model, params['val_dataloader'], test_mode = test_mode)\n",
    "    \n",
    "        for name, metric in metrics.items():\n",
    "            m = metric(val_labels, val_logits)\n",
    "            history['val_'+name].append(m)\n",
    "            print(\"Validation {}: {}\".format(name, m))\n",
    "        \n",
    "        ## Saving the history file if training is interrupted early\n",
    "        with open(logdir+'history.pk', 'wb') as f:\n",
    "            f.truncate(0)\n",
    "            pickle.dump(history, f)\n",
    "                \n",
    "        ## Saving best (wrt monitored quantity) and also the last model \n",
    "        monitored = history['val_accuracy']\n",
    "        best_value = max(monitored)\n",
    "        last_value = monitored[-1]\n",
    "        is_last_best = last_value == best_value\n",
    "        \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict' : optimizer.state_dict(),\n",
    "            'best_kpi': best_value,\n",
    "            'loss': history['train_loss_per_epoch'][-1],\n",
    "            #'params': params\n",
    "        }, is_last_best, logdir)\n",
    "   \n",
    "    return history\n",
    "\n",
    "def testEval(model, dataloader, test_mode=False):\n",
    "    ## eval mode!\n",
    "    model.eval()\n",
    "\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    val_logits = []\n",
    "    val_labels = []\n",
    "\n",
    "    for batch in tqdm_notebook(dataloader):\n",
    "        ## Add batch to GPU and unpack\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        ## not calculating the gradients speeds up inference a lot\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        ## logits and labels to CPU\n",
    "        logits = logits[0].detach().cpu().tolist()\n",
    "        label_ids = b_labels.to('cpu').tolist()\n",
    "\n",
    "        val_logits += logits\n",
    "        val_labels += label_ids\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "        ## if testing break early\n",
    "        if test_mode and nb_eval_steps > 20:\n",
    "            break   \n",
    "            \n",
    "    return val_labels, val_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train((model, optimizer, scheduler), params, test_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks about the hyper-parameters:\n",
    "* try out different learning rate schedules (cyclic works best a lot of times) but be careful about large values and catastrophic forgetting;\n",
    "* freeze and unfreeze layers, and adapt the learning rate to decrease on lower layers;\n",
    "* the recommended batch size for BERT fine-tuning is 16 or 32 (memory is also a big issue);\n",
    "* the model should be done in 4-6 epoch usually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_history(history, metrics):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(history['train_loss_per_batch'])\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Train loss and val acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss/Acc\")\n",
    "    for name in metrics.keys():\n",
    "        plt.plot(history['val_'+name])\n",
    "    plt.legend(['Train loss']+ [name for name in metrics.keys()])\n",
    "    plt.show()\n",
    "    \n",
    "draw_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, (finding the best hyper parameters and retraining the model a dozen times) we can reload the best model and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _ = loadModelTokenizer(4, 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ...\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = loader(test_ids, test_mask, test_labels)\n",
    "\n",
    "labels, logits = testEval(model, test_loader)\n",
    "\n",
    "metrics = {'accuracy': softmax_accuracy, 'roc_auc': softmax_rocauc}\n",
    "\n",
    "for name, metric in metrics.items():\n",
    "    m = metric(val_labels, val_logits)\n",
    "    print(\"Test {}: {}\".format(name, m))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
